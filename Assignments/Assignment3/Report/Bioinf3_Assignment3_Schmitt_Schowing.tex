\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}

% \usepackage{ngerman}  % german documents
\usepackage{graphicx}  % import graphics einbinden
\usepackage{listings}  % support source code listing
\usepackage{amsmath}  % math stuff
\usepackage{amssymb} % 
\usepackage{a4wide} % wide pages
\usepackage{fancyhdr} % nice headers
\usepackage{float}
\lstset{basicstyle=\footnotesize,language=Python,breaklines=true,numbers=left, numberstyle=\tiny, stepnumber=5,firstnumber=0, numbersep=5pt} % set up listings
\pagestyle{fancy}             % header
\setlength{\parindent}{0pt}   % no indentation

\usepackage[pdfpagemode=None, colorlinks=true,  % url coloring
           linkcolor=blue, urlcolor=blue, citecolor=blue, plainpages=false, 
           pdfpagelabels,unicode]{hyperref}
           
% change enums style: first level (a), (b), (c)           
\renewcommand{\labelenumi}{(\alph{enumi})}
\renewcommand{\labelenumii}{(\arabic{enumii})}

%lecture name
\newcommand{\lecture}{
	Bioinformatics III
}           

%assignment iteration
\newcommand{\assignment}{
	Third Assignment
}


%set up names, matricle number, and email
\newcommand{\authors}{
  \begin{tabular}{rl}
    \href{mailto:s8tbscho@stud.uni-saarland.de}{Thibault Schowing} & (2571837)\\
    \href{mailto:wiebkeschmitt@outlook.de}{Wiebke Schmitt} & (2543675)
  \end{tabular}
}

% use to start a new exercise
\newcommand{\exercise}[1]
{
  \stepcounter{subsection}
  \subsection*{Exercise \thesubsection: #1}

}

\begin{document}
\title{\Large \lecture \\ \textbf{\normalsize \assignment}}
\author{\authors}

\setlength \headheight{25pt}
\fancyhead[R]{\begin{tabular}{r}\lecture \\ \assignment \end{tabular}}
\fancyhead[L]{\authors}


\setcounter{section}{3} % modify for later sheets, i.e. 2, 3, ...
%\section{Introduction to Python and some Network Properties} % optional, note that section invocation sets the section counter + 1, so adapt the setcounter command
\maketitle

%WIEBKE !!! READ THIS !!! 
% Usefull with Latex and maths: a list of the symbols ! https://reu.dimacs.rutgers.edu/Symbols.pdf

%EXERCICE 1
\exercise{}
\begin{enumerate}

% A
\item \textit{Given the states of the features, you want to infer if two proteins are likely to physically
	interact. In practice, log-likelihood ratios are used in binary classification:}\\

\[ 
	log\frac{P(C|S)}{P(\bar{C}|S)} 
\]

\textit{Derive a term that uses observable probabilities such as $ P(S_i|C) $ to calculate the loglikelihood
	ratio from training data. How does the actual classification work?}\\



First we have: 

\[ P(S_i | C) = \frac{P(C|S_i)P(S_i)}{P(C)} \]

And: 

\[ P(S_i | \bar{C}) = \frac{P(\bar{C}|S_i)P(S_i)}{P(\bar{C})} \]

Then we develop the desired final output

\[ \frac{P(S_i|C)}{P(S_i |\bar{C})}  \Longleftrightarrow \frac{P(S_i | C)P(C)}{P(S_i |\bar{C})P(\bar{C})} = \frac{P(C|S_i)P(S_i)}{P(\bar{C}|S_i)P(S_i)} = \frac{P(C|S_i )}{P(\bar{C}|S_i)}\]



\[ log\frac{P(C|S)}{P(\bar{C}|S)} = log \prod_{i}^{n} \frac{P(S_i | C)P(C)}{P(S_i |\bar{C})P(\bar{C})} = \sum_{i}^{n} log \frac{P(S_i | C)P(C)}{P(S_i |\bar{C})P(\bar{C})} = \Lambda(C|S) \]

\[ O(C|S) =  \Lambda(C|S) O(C)\]

The posterior odd is calculated by the odds of an event ($ \frac{p(event)}{1-p(event)}$) multiplied by the likelihood of that event\footnote{Slides V4 - 4}. \\

To do the classification, we must interate through the data and calculate all the priors and likelihood. The prior $P(C)$ is made from an educated guess 
%TODO understand and complete


% B
\item \textit{Shortly discuss: What are the practical advantages of the logarithm and the likelihood ratio
	within this framework? State two reasons why this particular type of classifier may perform
	poorly on a real world dataset.}\\

% https://academic.oup.com/aje/article/153/12/1222/124010
% http://slideplayer.com/slide/5178261/
The logarithm increase is a monotonically increasing function of $x$ hence, for any positive value the maximum value of a function $f(x)$, the maximum of $f(x)$ is equal to the maximum of $log(f(x))$. This simplifies the calculation because we don't need the second derivative. A likelihood function is not concave but the log-likelihood is. Also, as seen in part A, with the log-likelihood we can turn a log of products into a sum of logs. The main inconvenient is that this method assume that all the features are independent and do not take in account the eventual correlations between them. 




% C
\newpage
\item \textit{Use the file training1.tsv to build a model. This basically means to determine all necessary
	priors and likelihoods from part (a). The file layout is explained in README.txt. Report
	$P(C)$ and $P(\bar{C})$ as well as the ten $S_i$ (feature number, variant and log-ratio) with the highest
	absolute log-likelihood ratios. Examine and comment on the results of the training-phase.
	Which features seem to be the most helpful?}\\

Prior probability $P(C) = 0.78$\\
Prior probability $P(\bar{C}) = 0.22$\\

 



\begin{table}[H]
	\centering
	\caption{10 $S_i$ with the highest absolute log-likelihood ratio}
	\label{10most}
	\begin{tabular}{lll}
		33 & 0 & -3.7214026458194964 \\
		11 & 3 & -2.565631943311438  \\
		87 & 1 & -2.4686396773241284 \\
		53 & 1 & -2.3351082846996056 \\
		99 & 1 & -2.3061207478263537 \\
		59 & 1 & -2.2779498708596573 \\
		80 & 2 & -2.2779498708596573 \\
		86 & 3 & -2.2550655770260692 \\
		91 & 3 & -2.2173252490432223 \\
		97 & 1 & -2.2099451417455995
	\end{tabular}
\end{table}

\lstinputlisting[label=ex2-b, caption={bayes.py}] {../Scripts/bayes.py}


\end{enumerate}



% NEW EXERCICE
\newpage
\exercise{Classify real-world network examples}
\begin{enumerate}
	\item 
	
	
	\item 
	
	
	\item 

\end{enumerate}

\end{document}